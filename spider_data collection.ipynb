{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection: 1st try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "page = requests.get('http://live.chinaz.com/anchor.html?platform=all&gameType=all&timeType=thismonth&date=20180325&method=power')\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# Name & Platform\n",
    "artist_name_list = soup.find_all(class_='info_avator')\n",
    "artist_name = []\n",
    "artist_platform = []\n",
    "\n",
    "for i in range(len(artist_name_list)):\n",
    "    artist_name.append(artist_name_list[i].find('span').contents[0])\n",
    "    artist_platform.append(artist_name_list[i].find('i').contents[0])\n",
    "\n",
    "    \n",
    "# Classification\n",
    "artist_classification_list = soup.find_all(class_='info_platform')\n",
    "artist_classification = []\n",
    "\n",
    "for i in range(len(artist_classification_list)):\n",
    "    artist_classification.append(artist_classification_list[i].find('span').contents[0])\n",
    "\n",
    "\n",
    "# URL in live.chinaz.com\n",
    "url_livechinaz_list = soup.find_all(class_='info')\n",
    "url_livechinaz = []\n",
    "\n",
    "for i in range(len(url_livechinaz_list)):\n",
    "    url_livechinaz.append(url_livechinaz_list[i].find('a').get('href'))\n",
    "    \n",
    "    \n",
    "url_artist = []\n",
    "for i in range(len(url_livechinaz)):\n",
    "    try:\n",
    "        page2 = requests.get(url_livechinaz[i])\n",
    "        soup2 = BeautifulSoup(page2.text, 'html.parser')\n",
    "\n",
    "        # URL of the artist\n",
    "        url_artist_list = soup2.find(class_='video clear')\n",
    "        url_artist.append(url_artist_list.find('a').get('href')[:-16])\n",
    "    except requests.Timeout as err:\n",
    "        url_artist.append(err.message)\n",
    "        \n",
    "        \n",
    "# CSV\n",
    "with open('APMA4990_TEST.csv','w',newline='',encoding='utf-8-sig') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['名字', '平台','类别','数据库地址'])\n",
    "    for i in range(50):\n",
    "        w.writerow([artist_name[i], artist_platform[i], artist_classification[i], url_artist[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. problem with time delay\n",
    "2. can only get first 50 streamer\n",
    "\n",
    "reference:\n",
    "[1] https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3 \n",
    "[2] https://www.cnblogs.com/xinyangsdut/p/7617691.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# 使用Chrome浏览器创建浏览器对象\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "\n",
    "# Initialize the driver\n",
    "driver = webdriver.Chrome(chrome_options=options)\n",
    "\n",
    "# 使用get方法加载页面\n",
    "driver.get(\"http://live.chinaz.com/anchor.html?platform=all&gameType=all&timeType=thismonth&date=20180325&method=power\")\n",
    "\n",
    "# 展开页面，提取n个数据\n",
    "for i in range(1,8):\n",
    "    # class=\"zhubo_btn\"是下一页按钮，click() 是模拟点击\n",
    "    driver.find_element_by_class_name('zhubo_btn').click()\n",
    "    # 给3s响应时间\n",
    "    time.sleep(5)\n",
    "    \n",
    "soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "# Name & Platform\n",
    "artist_name_list = soup.find_all(class_='info_avator')\n",
    "artist_name = []\n",
    "artist_platform = []\n",
    "\n",
    "#从info_avator抓出了一堆文字，挑选span里面的是主播名，i是平台名\n",
    "for m in range(len(artist_name_list)):\n",
    "    try:\n",
    "        artist_name.append(artist_name_list[m].find('span').contents[0])\n",
    "        artist_platform.append(artist_name_list[m].find('i').contents[0])\n",
    "    except:\n",
    "        print(m+1) # 出错数据的排名\n",
    "        artist_name.append('Error Name')\n",
    "        artist_platform.append('Error Platform')\n",
    "        \n",
    "# Rank\n",
    "artist_rank_list = soup.find_all(class_='rank')\n",
    "artist_rank = []\n",
    "\n",
    "for m in range(len(artist_rank_list)):\n",
    "    artist_rank.append(artist_rank_list[m].find('div').contents[0])\n",
    "\n",
    "for m in range(50):\n",
    "    artist_rank[m] = str(m+1)\n",
    "    \n",
    "# Classification\n",
    "artist_classification_list = soup.find_all(class_='info_platform')\n",
    "artist_classification = []\n",
    "\n",
    "for i in range(len(artist_classification_list)):\n",
    "    try:\n",
    "        artist_classification.append(artist_classification_list[i].find('span').contents[0])\n",
    "    except:\n",
    "        print(i+1) # 出错数据的排名\n",
    "        artist_classification.append('Error Classification')\n",
    "        \n",
    "# URL in live.chinaz.com\n",
    "url_livechinaz_list = soup.find_all(class_='info')\n",
    "url_livechinaz = []\n",
    "\n",
    "for i in range(len(url_livechinaz_list)):\n",
    "    try:\n",
    "        url_livechinaz.append(url_livechinaz_list[i].find('a').get('href'))\n",
    "    except:\n",
    "        print(i+1) # 出错数据的排名\n",
    "        url_livechinaz.append('Error URL in live.chinaz.com')\n",
    "        \n",
    "# CSV\n",
    "with open('APMA4990_TEST_500.csv','w',newline='',encoding='utf-8-sig') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['排名', '名字', '平台','类别','数据库地址'])\n",
    "    for i in range(len(url_livechinaz)):\n",
    "        w.writerow([artist_rank[i],artist_name[i], artist_platform[i], artist_classification[i], url_livechinaz[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ChromeDriver,get 400 data \n",
    "2. No.392: timeout error \n",
    "3. add rank to quickly check \n",
    "4. time.sleep is important! or will result in problem of showing\n",
    "\n",
    "Reference \n",
    "[1] https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3 \n",
    "[2] https://www.cnblogs.com/xinyangsdut/p/7617691.html \n",
    "[3] https://intoli.com/blog/running-selenium-with-headless-chrome/ \n",
    "[4] https://developers.google.com/web/updates/2017/04/headless-chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\Owner\\Python\\APMA4990_TEST_1950.csv',index_col=None)\n",
    "df.head()\n",
    "\n",
    "# URL of the artist\n",
    "url_artist = []\n",
    "\n",
    "for i in range(1950):\n",
    "    try:\n",
    "        page = requests.get(df.loc[i,'数据库地址'])\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        url_artist_list = soup.find(class_='video clear')\n",
    "        url_artist.append(url_artist_list.find('a').get('href')[:-16])\n",
    "    except:\n",
    "        url_artist.append('Error')\n",
    "        \n",
    "# CSV\n",
    "with open('APMA4990_TEST_4_9.csv','w',newline='',encoding='utf-8-sig') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['排名', '主播', '平台','类别','直播间地址'])\n",
    "    for i in range(len(url_artist)):\n",
    "        w.writerow([df.loc[i,'排名'],df.loc[i,'名字'], df.loc[i,'平台'], df.loc[i,'类别'], url_artist[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
